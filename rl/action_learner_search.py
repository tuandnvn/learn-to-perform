# -*- coding: utf-8 -*-
from __future__ import print_function
import sys
    
import tensorflow as tf
import numpy as np
import collections
import itertools

from . import uniform_env_space
from . import block_movement_env as bme
import plotting
import traceback

from gym.wrappers import TimeLimit
from gym.utils import seeding
# from importlib import reload
# reload(bme)

def random_action_constraint(state, policy_estimator, no_of_actions = 1, verbose = False, 
       session = None, constraint_function = lambda a : True):
    """
    Random no_of_actions that satisfy constraints specified by constraint_function
    """
    action_means, action_stds = policy_estimator.predict(state, sess = session)

    variances = action_stds ** 2

    actions = []

    while True:
        tempo = np.random.multivariate_normal(action_means,np.diag(variances), size = no_of_actions)

        actions += [act for act in tempo if constraint_function(act)]

        if len(actions) > no_of_actions:
            break

    return action_means, action_stds, actions[:no_of_actions]

def action_policy(config):
    """
    Given a config that has defined a playground
    """
    def boundary_constraint(action):
        # Ignore rotation
        for i in range(2):
            if action[i] < config.playground_x[i]:
                return False
            if action[i] > config.playground_x[i] + config.playground_dim[i]:
                return False
        
        return True
    
    def q(state, policy_estimator, no_of_actions = 1, verbose = False, 
       session = None):
        return random_action_constraint(state, policy_estimator,
                    no_of_actions, verbose, session, boundary_constraint)
    
    return q

class PolicyEstimator():
    """
    Policy Function approximator.
    
    A policy estimator is a function that produce the following distribution
    π(a|s, θ)
    
    s is the current state
    a is an action that has been selected from the action distribution 
    θ is the set of parameters, in this case, these are a neural network weight
    
    Each time, we have two inputs:
        - A state generated by Monte Carlo
        - An action generated by Monte Carlo
    The model would produce (from the state) an output layer
    that correspond to a preference function h(s,a).
    
    Following is the formula for updating:
    
    θ += α*γ^t*G * DELTA_θ (log π(At|St, θ))
    
    or, more succintly, for baseline also:
    
    θ += α * TARGET_WEIGHT * DELTA_θ (log π(At|St, θ))
    
    where TARGET_WEIGHT could be the (discounted) return from time t
    adjusted with baseline.
    
    This corresponding to the following graph:
    
    The graph basically predicts the action probability, BUT the loss function
    needs to be reweighted with the TARGET_WEIGHT, so that the updating 
    follows the formula
    """
    
    def __init__(self, config, scope="policy_estimator"):
        """
        The code to declare your tensorflow graph comes here
        """

        # This state dimension would probably be 12
        # location + rotation of two most objects
        state_dimension = config.state_dimension

        # This would be 3. 2 for locations, 1 for rotation
        action_dimension =  config.action_dimension

        # sigma_dimension is simplified to 2
        # in a full model, this value would be 9
        # taking in all covariances between all variables
        # In this model, we simplify that to a diagonal matrix
        # which means we actually generate each value independently
        # Covariance matrix  = [ sigma_1, 0, 0 ]
        #                      [ 0, sigma_2, 0 ]
        #                      [ 0, 0, sigma_3 ]
        sigma_dimension = config.action_dimension

        # short for weight_regularizer_scale
        wrs = config.weight_regularizer_scale

        self.lr = tf.Variable(0.0, trainable=False)

        """
        After trying to learn sigma value, we might want to give up a
        and just set it value for each episode
        """
        self.sigma_layer = tf.Variable([1,1,1], dtype = tf.float32, trainable=False)

        hidden_size = 10
        
        with tf.variable_scope(scope): 
            "Declare all placeholders"
            "Placeholder for input"
            """
            Now in this case, if the state is represented as a number ranking from
            start point of the map to the end, we lost the locality between
            cells of two consecutive rows.
            So let's make it a row and column.
            """
            # No batch
            self.state = tf.placeholder(shape=[state_dimension], name="state", dtype = tf.float32)
            
            "Placeholder for Monte Carlo action"
            self.action = tf.placeholder(shape=[action_dimension], name="action", dtype = tf.float32)
            
            "Placeholder for target"
            self.target = tf.placeholder(name="target", dtype = tf.float32)

            
            """
            mu_layer is a fully connected layer, produce location/rotation of the action
            
            activation_fn=None because we want a linear function

            Currently the whole problem is that a linear function might not be helpful to learn 
            this kind of problem
            """
            hidden_layer = tf.squeeze(tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(self.state, 0),
                num_outputs=hidden_size,
                activation_fn=tf.nn.sigmoid,
                weights_initializer=tf.zeros_initializer()))

            self.mu_layer = tf.squeeze(tf.contrib.layers.fully_connected(
                inputs=tf.expand_dims(hidden_layer, 0),
                num_outputs=action_dimension,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer()))
            
            """
            Using softplus so that the output would be > 0 but we also don't want 0
            We look for sigma value ~ 0.2
            """
            # self.sigma_layer = 0.2 * tf.squeeze(tf.contrib.layers.fully_connected(
            #     inputs=state_expanded,
            #     num_outputs=sigma_dimension,
            #     activation_fn=tf.nn.sigmoid,
            #     weights_initializer=tf.random_uniform_initializer(minval=1.0/(5 * state_dimension), maxval=2.0/(5 * state_dimension))))

            # Using a mvn to predict action probability
            mvn = tf.contrib.distributions.Normal(
                loc=self.mu_layer,
                scale=self.sigma_layer)

            # (action_dimension)
            self.picked_action_prob = mvn.prob(self.action) 
            
            #print (tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
            self.regularizer_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))

            self.sigma_constraint = tf.norm(self.sigma_layer)

            # The action probability is the product of component probabilities
            # Notice that the formula for REINFORCE update is (+) gradient of log-prob function
            # so we minimize the negative log-prob function instead
            self.loss = -tf.reduce_sum(tf.log(self.picked_action_prob)) * self.target + config.constraint_sigma * self.sigma_constraint
            
            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)
            
            self.train_op = self.optimizer.minimize(
                self.loss, global_step=tf.contrib.framework.get_global_step())
    
    def predict(self, state, sess=None):
        """
        In prediction, just need to produce the multivariate distribution
        """
        sess = sess or tf.get_default_session()
        return sess.run([self.mu_layer, self.sigma_layer], {self.state: state})
    
    def update(self, state, target, action, sess=None):
        """
        state: input state
        target: return from time t of episode * discount factor
        action: input action
        
        We need to run train_op to update the parameters
        We also need to return its loss
        """
        sess = sess or tf.get_default_session()
        _, loss, regularizer_loss = sess.run([self.train_op, self.loss, self.regularizer_loss], {self.state: state, self.action: action, self.target: target})
        
        return loss, regularizer_loss

    def assign_lr(self, lr_value, sess=None):
        sess = sess or tf.get_default_session()
        
        sess.run(tf.assign(self.lr, lr_value))

    def assign_sigma(self, sigma_value, sess=None):
        sess = sess or tf.get_default_session()
        
        sess.run(tf.assign(self.sigma_layer, sigma_value))

class ActionLearner_Search(object):
    """
    This search method starts is similar to REINFORCE,
    but with a twist.
    - Firstly, it starts with a large standard deviation (say 2) so that all points in the play ground have similar distribution
    - For each start configuration, it runs a thorough search, with a breath. For example, breadth = 10 -> it randomizes at least
    10 legal actions. For each next move, it randomizes another 10 actions, for 100 configurations, than it selects the best 10 for 
    next search.
    - In general, the algorithm would stop increase action-steps when there are a trajectory that satisfy progress function > threshold.
    - It keeps searching over the remaining space for possibly more trajectory. 
    - All "good" trajectories are kept as update samples for the policy.

    - The target is for the agent to quickly find a good policy 
    - When the number of actions need to search doesn't improve at a step
    - We might want to split the gaussian models into a gaussian mixture model 
    
    """
    def __init__(self, config, project, progress_estimator, limit_step = 10, session = None, env = None):
        self.config = config

        # All of these components should be 
        # This should belong to class Project
        # We assume that the data put in the project here has been preprocessed
        self.project = project

        # This should be a kind of class EventProgressEstimator
        # We assume that the progress_estimator put in the project has been learned
        self.progress_estimator = progress_estimator

        # This should belong to class PolicyEstimator
        with tf.variable_scope("search", reuse = True) as scope:
            self.policy_estimator = PolicyEstimator(self.config)

        self.limit_step = limit_step

        self.session = session

        self.np_random, _ = seeding.np_random(None)

        if env == None:
            env = bme.BlockMovementEnv(self.config, self.project.speed, self.project.name, 
                progress_estimator = self.progress_estimator, session = self.session)
            env.reset()
        
        self.env = env

        self.action_policy = action_policy(self.config)

    def learn_one_setup( self, select_object = 0, verbose = False):
        sigma = self.config.start_sigma
        self.policy_estimator.assign_sigma( sigma, sess= self.session )

        # Every action_level, we would search for keep_branching * branching new positions
        # keep_branching is the number of explorations keep from the previous step
        # For the first action_level, keep_branching = 1
        #  branching is the number of new action explored for each exploration
        # For the first action_level, keep_branching = keep_branching * branching
        keep_branching = self.config.keep_branching
        branching = self.config.branching
        # shorten
        env = self.env

        explorations = [env.clone()]

        # Each accumulated reward for each exploration
        rewards = [0]

        found_completed_act = False
        # We do one action at a time for all exploration
        for action_level in range(3):
            if verbose:
                print ('action_level = %d' % action_level)
        
            # This would store a tuple of (exploration_index, accumulated_reward, action, action_means, action_stds)
            # branching ** 2
            tempo_rewards = []
            
            for exploration_index, exploration in enumerate(explorations):
                if verbose:
                    print ('exploration_index = %d' % exploration_index)


                if action_level == 0:
                    no_of_search = keep_branching * branching
                    state = exploration.get_observation_start()
                else:
                    no_of_search = branching
                    # State interpolated by WHOLE mode
                    state, _ = exploration.get_observation_and_progress()
                #print ('state = ' + str(state))

                action_means, action_stds, actions = self.action_policy(state, self.policy_estimator,
                    verbose = verbose, no_of_actions = no_of_search, session = self.session)

                #print (actions)

                for action_index, action in enumerate(actions):
                    _, reward, done, _ = exploration.step((select_object,action, action_means, action_stds))
                    #print ((action, reward))
                    exploration.back()

                    tempo_rewards.append( (exploration_index, rewards[exploration_index] + reward,
                        action, action_means, action_stds) )

                    if done:
                        print ("=== found_completed_act ===")
                        found_completed_act = True

                # tuple_actions = [(select_object, action, action_means, action_stds) for action in actions]
                # legal_action_indices, all_progress = exploration.try_step_multi(tuple_actions)

                # for index, progress in zip (legal_action_indices, all_progress):
                #     tempo_rewards.append( (exploration_index, progress,
                #         actions[index], action_means, action_stds) )

                #     if progress > self.config.progress_threshold:
                #         print ("=== found_completed_act ===")
                #         found_completed_act = True

            tempo_rewards = sorted(tempo_rewards, key = lambda t: t[1], reverse = True)
            test = [(t[0], t[1]) for t in tempo_rewards]

            if verbose:
                print (test[:keep_branching])

            new_explorations = []
            rewards = []
            for exploration_index, acc_reward, action, action_means, action_stds in tempo_rewards[:keep_branching]:
                env = explorations[exploration_index].clone()
                env.step((select_object,action, action_means, action_stds))
                new_explorations.append(env)
                rewards.append(acc_reward)
            
            explorations = new_explorations

            if found_completed_act:
                # Stop increase action_level
                break

        return explorations

    def learn( self , action_policy, verbose = False):
        select_object = 0

        sigma = self.config.start_sigma
        branching = self.config.branching

        # We want to keep these envs for the next loop
        train_envs = []

        for i in range(self.config.no_of_start_setups):
            env = bme.BlockMovementEnv(self.config, self.project.speed, self.project.name, 
                progress_estimator = self.progress_estimator, session = self.session)
            env.reset()
            train_envs.append(env)

        # These will be used to test the performance of our learned policy
        test_envs = []

        for i in range(self.config.no_of_test_setups):
            env = bme.BlockMovementEnv(self.config, self.project.speed, self.project.name, 
                progress_estimator = self.progress_estimator, session = self.session)

            test_envs.append(env)


        for i_loop in range(self.config.no_of_loops):
            if verbose:
                print ('Current loop index = %d' % i_loop)

            # At any time, we only keep self.config.branching of 
            # explorations for each train_env_index
            explorations = {}
            for train_env_index, train_env in enumerate(train_envs):
                explorations[train_env_index] = [train_env.clone()]

            
            # Traverse through each train_env_index
            for train_env_index in explorations.keys():
                print ('train_env_index = %d' % train_env_index)

                # Each accumulated reward for each exploration
                rewards = [0]

                found_completed_act = False
                # We do one action at a time for all exploration
                for action_level in itertools.count():
                    if verbose:
                        print ('action_level = %d' % action_level)
                
                    # This would store a tuple of (exploration_index, accumulated_reward, action, action_means, action_stds)
                    # branching ** 2
                    tempo_rewards = []
                    
                    for exploration_index, exploration in enumerate(explorations[train_env_index]):
                        if verbose:
                            print ('exploration_index = %d' % exploration_index)

                        if action_level == 0:
                            no_of_search = branching ** 2
                            state = exploration.get_observation_start()
                        else:
                            no_of_search = branching
                            # State interpolated by WHOLE mode
                            state, _ = exploration.get_observation_and_progress()
                        #print ('state = ' + str(state))

                        action_means, action_stds, actions = action_policy(state, self.policy_estimator,
                            verbose = verbose, no_of_actions = no_of_search, session = self.session)

                        for action_index, action in enumerate(actions):
                            _, reward, done, _ = exploration.step((select_object,action, action_means, action_stds))
                            #print ((action, reward))
                            exploration.back()

                            tempo_rewards.append( (exploration_index, rewards[exploration_index] + reward,
                                action, action_means, action_stds) )

                            if done:
                                print ("found_completed_act found_completed_act found_completed_act")
                                found_completed_act = True

                    tempo_rewards = sorted(tempo_rewards, key = lambda t: t[1], reverse = True)
                    test = [(t[0], t[1]) for t in tempo_rewards]

                    if verbose:
                        print (test[:branching])

                    new_explorations = []
                    rewards = []
                    for exploration_index, acc_reward, action, action_means, action_stds in tempo_rewards[:branching]:
                        env = explorations[train_env_index][exploration_index].clone()
                        env.step((select_object,action, action_means, action_stds))
                        new_explorations.append(env)
                        rewards.append(acc_reward)
                    
                    explorations[train_env_index] = new_explorations

                    if found_completed_act:
                        # Stop increase action_level
                        break

            sigma *= self.config.sigma_discount_factor

            # Updating policy estimator with set of good explorations
