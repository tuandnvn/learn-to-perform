{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Wrappers are used to transform an environment in a modular way\n",
    "# It is just to add extensional functionalities\n",
    "# Monitor: That could help to record a video on an episode!\n",
    "# That is really nice!!!!\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import plotting\n",
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 4\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "Observation space shape: (210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADntJREFUeJzt3X/sVfV9x/Hna1hNRruI9UcM4ABH\n2+myUUscmdN0c7VImqJL2kGWyjYzNJGkjS4Z1mQjS5psXcGk2UaDkRQXC7pRK1mshbCmZtmwgkWE\nIgqU1q8QmLiIw6YOeO+P8/mm1y/fy/dy3+f2nnt9PZKbe+/nnnPP+wRefM49nPu+igjMrHu/1O8C\nzAadQ2SW5BCZJTlEZkkOkVmSQ2SW1LMQSZovaZ+k/ZKW92o7Zv2mXvw/kaRJwMvAJ4AR4DlgcUT8\nsPaNmfVZr2ai64H9EXEwIt4BNgALe7Qts766oEfvOxV4teX5CPDb7RaW5MsmrIlej4jLJlqoVyHS\nOGPvCoqkpcDSHm3frA4/7mShXoVoBJje8nwacLh1gYhYA6wBz0Q22Hr1meg5YLakmZIuBBYBm3q0\nLbO+6slMFBGnJC0DvgNMAtZGxJ5ebMus33pyivu8i2jg4dyqVavOe51777039R5j16/rPbKaUMNY\nY2vq0TZ3RMTciRbyFQtmSb06sTB0ejFL9GO2q8MvYqYZJJ6JzJI8E9l5m2j2e6/NVJ6JzJI8E9mE\nJppZ+vG5rEk8E5kleSbqUB3/2jblPQZhm4PEM5FZkkNkluTLfsza82U/Zr8IjTixMG3atPfcf9BZ\n83X6d9IzkVmSQ2SW5BCZJTlEZkldh0jSdEnflbRX0h5Jny/jKyS9JmlnuS2or1yz5smcnTsF3BcR\nz0v6ALBD0pby2oMR8ZV8eWbN13WIIuIIcKQ8fkvSXqqmjWbvKbV8JpI0A/go8GwZWiZpl6S1kqbU\nsQ2zpkqHSNL7gY3AFyLiBLAauBqYQzVTrWyz3lJJ2yVtP3nyZLYMs75JhUjS+6gC9GhEfBMgIo5G\nxOmIOAM8RNXc/iwRsSYi5kbE3MmTJ2fKMOurzNk5AQ8DeyNiVcv4lS2L3Q7s7r48s+bLnJ27Afgc\n8KKknWXsi8BiSXOoGtgfAu5KVWjWcJmzc//B+L/+8FT35ZgNHl+xYJbUiK9CTMRfk7BeqKt3hGci\nsySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLSn+f\nSNIh4C3gNHAqIuZKugR4DJhB9RXxz0bE/2S3ZdZEdc1EvxcRc1p+VWw5sDUiZgNby3OzodSrw7mF\nwLryeB1wW4+2Y9Z3dYQogM2SdkhaWsauKG2GR9sNX17DdswaqY4eCzdExGFJlwNbJL3UyUolcEsB\npkxxp2EbXOmZKCIOl/tjwBNUHU+PjjZxLPfHxlnPHVBtKGTbCE8uP6uCpMnALVQdTzcBS8piS4An\nM9sxa7Ls4dwVwBNVR2EuAL4REU9Leg54XNKdwE+AzyS3Y9ZYqRBFxEHgt8YZPw7cnHlvs0HhKxbM\nkgaiA+q2+fP7XYINof+s6X08E5klOURmSQ6RWZJDZJbkEJklDcTZuTO/dqLfJZi15ZnILMkhMkty\niMySHCKzJIfILMkhMksaiFPcb/zK2/0uwawtz0RmSQ6RWVLXh3OSPkzV5XTULOCvgIuBPwf+u4x/\nMSKe6rpCs4brOkQRsQ+YAyBpEvAaVbefPwUejIiv1FKhWcPVdTh3M3AgIn5c0/uZDYy6zs4tAta3\nPF8m6Q5gO3Bftpn9Gx95J7O62fher+dt0jORpAuBTwP/UoZWA1dTHeodAVa2WW+ppO2Stp88eTJb\nhlnf1HE4dyvwfEQcBYiIoxFxOiLOAA9RdUQ9izug2rCoI0SLaTmUG20fXNxO1RHVbGilPhNJ+mXg\nE8BdLcNfljSH6tciDo15zWzoZDugvg18cMzY51IVmQ2Ygbh27htnrup3CTaEbqnpfXzZj1mSQ2SW\n5BCZJTlEZkkOkVnSQJyde2fDin6XYMPolnp+XMUzkVmSQ2SW5BCZJTlEZkkOkVmSQ2SWNBCnuP/9\n6Xn9LsGG0KduWVXL+3gmMktyiMySHCKzpI5CJGmtpGOSdreMXSJpi6RXyv2UMi5JX5W0X9IuSdf1\nqnizJuh0Jvo6MH/M2HJga0TMBraW51B1/5ldbkupWmiZDa2OQhQRzwBvjBleCKwrj9cBt7WMPxKV\nbcDFYzoAmQ2VzGeiKyLiCEC5v7yMTwVebVlupIy9i5s32rDoxYkFjTMWZw24eaMNiUyIjo4eppX7\nY2V8BJjestw04HBiO2aNlgnRJmBJebwEeLJl/I5ylm4e8OboYZ/ZMOrosh9J64GPA5dKGgH+Gvhb\n4HFJdwI/AT5TFn8KWADsB96m+r0is6HVUYgiYnGbl24eZ9kA7skUZTZIfMWCWZJDZJbkEJklOURm\nSQ6RWZJDZJbkEJklOURmSQ6RWZJDZJbkEJklOURmSQ6RWZJDZJbkEJklOURmSQ6RWdKEIWrT/fTv\nJb1UOpw+IeniMj5D0k8l7Sy3r/WyeLMm6GQm+jpndz/dAvxGRPwm8DJwf8trByJiTrndXU+ZZs01\nYYjG634aEZsj4lR5uo2qLZbZe1Idn4n+DPh2y/OZkn4g6XuSbmy3kjug2rBI/VKepAeAU8CjZegI\ncFVEHJf0MeBbkq6NiBNj142INcAagOnTp5/VIdVsUHQ9E0laAnwK+OPSJouI+FlEHC+PdwAHgA/V\nUahZU3UVIknzgb8EPh0Rb7eMXyZpUnk8i+rnVQ7WUahZU014ONem++n9wEXAFkkA28qZuJuAv5F0\nCjgN3B0RY3+SxWyoTBiiNt1PH26z7EZgY7Yos0HiKxbMkhwisySHyCzJITJLcojMkhwisySHyCzJ\nITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkrrtgLpC0mstnU4XtLx2v6T9kvZJ\n+mSvCjdrim47oAI82NLp9CkASdcAi4Bryzr/NNq4xGxYddUB9RwWAhtK66wfAfuB6xP1mTVe5jPR\nstLQfq2kKWVsKvBqyzIjZews7oBqw6LbEK0GrgbmUHU9XVnGNc6y43Y3jYg1ETE3IuZOnjy5yzLM\n+q+rEEXE0Yg4HRFngIf4+SHbCDC9ZdFpwOFciWbN1m0H1Ctbnt4OjJ652wQsknSRpJlUHVC/nyvR\nrNm67YD6cUlzqA7VDgF3AUTEHkmPAz+kanR/T0Sc7k3pZs1QawfUsvyXgC9lijIbJL5iwSzJITJL\ncojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwis6Rumzc+\n1tK48ZCknWV8hqSftrz2tV4Wb9YEE36zlap54z8Aj4wORMQfjT6WtBJ4s2X5AxExp64CzZquk6+H\nPyNpxnivSRLwWeD36y3LbHBkPxPdCByNiFdaxmZK+oGk70m6Mfn+Zo3XyeHcuSwG1rc8PwJcFRHH\nJX0M+JakayPixNgVJS0FlgJMmTJl7MtmA6PrmUjSBcAfAo+NjpUe3MfL4x3AAeBD463vDqg2LDKH\nc38AvBQRI6MDki4b/RUISbOomjcezJVo1mydnOJeD/wX8GFJI5LuLC8t4t2HcgA3AbskvQD8K3B3\nRHT6ixJmA6nb5o1ExJ+MM7YR2Jgvy2xw+IoFsySHyCzJITJLcojMkhwisySHyCzJITJLcojMkhwi\ns6TsVdy1eHPSGf7t4v/tdxk2jm3z56fWn/f00zVVUr/f2by5lvfxTGSW5BCZJTlEZkmN+ExkzdXk\nzzRN4ZnILMkzkb1n1TXLKiJqeaNUEVL/izA7246ImDvRQp18PXy6pO9K2itpj6TPl/FLJG2R9Eq5\nn1LGJemrkvZL2iXpuvy+mDVXJ5+JTgH3RcSvA/OAeyRdAywHtkbEbGBreQ5wK1WDktlULbFW1161\nWYNMGKKIOBIRz5fHbwF7ganAQmBdWWwdcFt5vBB4JCrbgIslXVl75WYNcV5n50o74Y8CzwJXRMQR\nqIIGXF4Wmwq82rLaSBkzG0odn52T9H6qTj5fiIgTVRvu8RcdZ+ysEwetHVDNBllHM5Gk91EF6NGI\n+GYZPjp6mFbuj5XxEWB6y+rTgMNj37O1A2q3xZs1QSdn5wQ8DOyNiFUtL20ClpTHS4AnW8bvKGfp\n5gFvjh72mQ2liDjnDfhdqsOxXcDOclsAfJDqrNwr5f6SsryAf6Tqw/0iMLeDbYRvvjXwtn2iv7sR\n4f9sNTuHev6z1czOzSEyS3KIzJIcIrMkh8gsqSnfJ3odOFnuh8WlDM/+DNO+QOf786udvFkjTnED\nSNo+TFcvDNP+DNO+QP3748M5sySHyCypSSFa0+8CajZM+zNM+wI1709jPhOZDaomzURmA6nvIZI0\nX9K+0thk+cRrNI+kQ5JelLRT0vYyNm4jlyaStFbSMUm7W8YGthFNm/1ZIem18me0U9KCltfuL/uz\nT9Inz3uDnVzq3asbMInqKxOzgAuBF4Br+llTl/txCLh0zNiXgeXl8XLg7/pd5znqvwm4Dtg9Uf1U\nX4P5NtVXXuYBz/a7/g73ZwXwF+Mse035e3cRMLP8fZx0Ptvr90x0PbA/Ig5GxDvABqpGJ8OgXSOX\nxomIZ4A3xgwPbCOaNvvTzkJgQ0T8LCJ+BOyn+nvZsX6HaFiamgSwWdKO0jsC2jdyGRTD2IhmWTkE\nXdtyeJ3en36HqKOmJgPghoi4jqrn3j2Sbup3QT00qH9mq4GrgTnAEWBlGU/vT79D1FFTk6aLiMPl\n/hjwBNXhQLtGLoMi1YimaSLiaEScjogzwEP8/JAtvT/9DtFzwGxJMyVdCCyianQyMCRNlvSB0cfA\nLcBu2jdyGRRD1YhmzOe226n+jKDan0WSLpI0k6pz7/fP680bcCZlAfAy1VmRB/pdTxf1z6I6u/MC\nsGd0H2jTyKWJN2A91SHO/1H9y3xnu/rpohFNQ/bnn0u9u0pwrmxZ/oGyP/uAW893e75iwSyp34dz\nZgPPITJLcojMkhwisySHyCzJITJLcojMkhwis6T/BzF6WOXJ/icoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18ec08a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12058fcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print (env)\n",
    "# <TimeLimit<AtariEnv<Breakout-v0>>>\n",
    "# TimeLimit is env wrapper that provides ...\n",
    "# max_episode_seconds, max_episode_steps\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "print(env.env.get_action_meanings())\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \n",
    "    It is a very interesting consideration to represent the whole system \n",
    "    as an image. It is understandable that the part corresponding to the \n",
    "    blocks should be represented like this, but \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            # The upper part is not relevant (there is some score)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            \n",
    "            # Why resize from 160 to 84\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84,84], \n",
    "                method = tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            \n",
    "            self.output = tf.squeeze(self.output)\n",
    "            \n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: a Tensorflow session object\n",
    "            state: A[210, 160, 3] Atari RGB state\n",
    "            \n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, {self.input_state: state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network\n",
    "    \n",
    "    This network is used for both the Q-network and the Target Network\n",
    "    \"\"\"\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir = None):\n",
    "        self.scope = scope\n",
    "        \n",
    "        # Write Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "            \n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "                \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \n",
    "        Network calculate Q(s,a) - action value of (state, action)\n",
    "        \"\"\"\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        # Why 4?\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # target result of the network\n",
    "        self.y_pl = tf.placeholder(shape=[None],dtype=tf.float32, name=\"y\")\n",
    "        \n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "        \n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        \n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "        \n",
    "        # This is a pretty big network\n",
    "        \n",
    "        # 64 filters of kernel size 8, stride of 4\n",
    "        # Size reduce to (84 - 8)/4 + 1 = (20, 20, 64)\n",
    "        conv1 = tf.contrib.layers.conv2d(X, 32, 8, 4, activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # Size reduce to (20 - 4)/2 + 1 = (9, 9, 64)\n",
    "        conv2 = tf.contrib.layers.conv2d(X, 64, 4, 2, activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # Size reduce to (9 - 3)/1 + 1 = (7, 7, 64)\n",
    "        conv3 = tf.contrib.layers.conv2d(X, 64, 3, 1, activation_fn = tf.nn.relu)\n",
    "        \n",
    "        # 7 * 7 * 64\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        \n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        \n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "        \n",
    "        # Get the predictions for the chosen actions only\n",
    "        # Normally I would use the gather_2d function\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        # http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "        # initial learning rate = 0.00025\n",
    "        # decay rate = 0.99\n",
    "        # momentum = 0.0\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        \n",
    "        self.train_op = self.optimizer.minimize(self.loss, \n",
    "                    global_step = tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "        \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuando/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05670894  0.          0.          0.10329727]\n",
      " [ 0.05670894  0.          0.          0.10329727]]\n",
      "98.9724\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    # exact states are replicated!\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    \n",
    "    # Print loss = 100, which is expectable\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    \n",
    "    # same parameter for each model at the same index\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    \n",
    "    # Assigning operators doesn't have any effect until you run in a session\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    # A better way to save checkpoint\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    # Why do we need something like this?\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    # This is a functional way to generate epsilons\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        # TODO: Populate replay memory!\n",
    "        \n",
    "        pass\n",
    "\n",
    "    # Record videos\n",
    "    env= Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                pass\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            # TODO: Implement!\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # TODO: Save transition to replay memory\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            # TODO: Calculate q values and targets\n",
    "            # TODO Perform gradient descent update\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuando/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-41-6e850d7206fe>:18: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Populating replay memory...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6e850d7206fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-4edecd5caec6>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Save the current checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Reset the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1389\u001b[0m       model_checkpoint_path = sess.run(\n\u001b[1;32m   1390\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m           {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1392\u001b[0m       \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=1,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
