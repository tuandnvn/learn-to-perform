import tensorflow as tf 

class PolicyEstimator():
    """
    Policy Function approximator.
    
    A policy estimator is a function that produce the following distribution
    π(a|s, θ)
    
    s is the current state
    a is an action that has been selected from the action distribution 
    θ is the set of parameters, in this case, these are a neural network weight
    
    Each time, we have two inputs:
        - A state generated by Monte Carlo
        - An action generated by Monte Carlo
    The model would produce (from the state) an output layer
    that correspond to a preference function h(s,a).
    
    Following is the formula for updating:
    
    θ += α*γ^t*G * DELTA_θ (log π(At|St, θ))
    
    or, more succintly, for baseline also:
    
    θ += α * TARGET_WEIGHT * DELTA_θ (log π(At|St, θ))
    
    where TARGET_WEIGHT could be the (discounted) return from time t
    adjusted with baseline.
    
    This corresponding to the following graph:
    
    The graph basically predicts the action probability, BUT the loss function
    needs to be reweighted with the TARGET_WEIGHT, so that the updating 
    follows the formula
    """
    
    def __init__(self, learning_rate=0.01, scope="policy_estimator", config):
        """
        The code to declare your tensorflow graph comes here
        """

        # This state dimension would probably be 12
        # location + rotation of two most objects
		state_dimension = config.state.dimension

		# This would be 3. 2 for locations, 1 for rotation
		action_dimension =  config.action.dimension

		# sigma_dimension is simplified to 2
		# in a full model, this value would be 9
		# taking in all covariances between all variables
		# In this model, we simplify that to a diagonal matrix
		# which means we actually generate each value independently
		# Covariance matrix  = [ sigma_1, 0, 0 ]
		#					   [ 0, sigma_2, 0 ]
		# 					   [ 0, 0, sigma_3 ]
		sigma_dimension = config.action.dimension

        with tf.variable_scope(scope): 
            "Declare all placeholders"
            "Placeholder for input"
            """
            Now in this case, if the state is represented as a number ranking from
            start point of the map to the end, we lost the locality between
            cells of two consecutive rows.
            So let's make it a row and column.
            """
            # No batch
            self.state = tf.placeholder(shape=[state_dimension], name="state", dtype = tf.float32)
            
            "Placeholder for Monte Carlo action"
            self.action = tf.placeholder(shape=[action_dimension], name="action", dtype = tf.int32)
            
            "Placeholder for target"
            self.target = tf.placeholder(name="target", dtype = tf.float32)
            
            state_expanded = tf.expand_dims(self.state, 0)
            """
            mu_layer is a fully connected layer, produce location/rotation of the action
            
            activation_fn=None because we want a linear function
            """
            self.mu_layer = tf.squeeze(tf.contrib.layers.fully_connected(
                inputs=state_expanded,
                num_outputs=action_dimension,
                activation_fn=None,
                weights_initializer=tf.zeros_initializer))
            
            """
            Using ReLu activation_fn so that the output would be non-negative
            """
            self.sigma_layer = tf.squeeze(tf.contrib.layers.fully_connected(
                inputs=state_expanded,
                num_outputs=sigma_dimension,
                activation_fn=tf.nn.relu,
                weights_initializer=tf.zeros_initializer))

            # Using a mvn to predict action probability
            mvn = tf.distributions.Normal(
			    loc=self.mu_layer,
			    scale=self.sigma_layer)

            # (action_dimension)
            self.picked_action_prob = mvn.prob(self.action) 
            
            # The action probability is the product of component probabilities
            # Notice that the formula for REINFORCE update is (+) gradient of log-prob function
            # so we minimize the negative log-prob function instead
            self.loss = -tf.sum(tf.log(self.picked_action_prob)) * self.target
            
            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            
            self.train_op = self.optimizer.minimize(
                self.loss, global_step=tf.contrib.framework.get_global_step())
    
    def predict(self, state, sess=None):
        """
        In prediction, just need to produce the multivariate distribution
        """
        sess = sess or tf.get_default_session()
        return sess.run([self.mu_layer, self.sigma_layer], {self.state: state})
    
    def update(self, state, target, action, sess=None):
        """
        state: input state
        target: return from time t of episode * discount factor
        action: input action
        
        We need to run train_op to update the parameters
        We also need to return its loss
        """
        sess = sess or tf.get_default_session()
        _, loss = sess.run([self.train_op, self.loss], {self.state: state, self.action: action, self.target: target})
        
        return loss


class ValueEstimator():
    """
    Value Function approximator.
    
    This is to calculate the baseline, otherwise Policy Estimator is enough
    
    We need another set of parameter w for state-value approximator.
    
    Target is (discounted) return
    
    Just use a very simple linear fully connected layer between state and output
    """
    
    def __init__(self, learning_rate=0.1, scope="value_estimator", config):
        # This state dimension would probably be 12
        # location + rotation of two most objects
		state_dimension = config.state.dimension

        with tf.variable_scope(scope): 
            # No batch
            self.state = tf.placeholder(shape=[state_dimension], name="state", dtype = tf.int32)
            
            "Placeholder for target"
            self.target = tf.placeholder(name="target", dtype = tf.float32)
            
            self.output_layer = tf.contrib.layers.fully_connected(
                tf.expand_dims(self.state,0),
                1,
                activation_fn=tf.nn.relu,
                weights_initializer=tf.zeros_initializer)
            
            self.value = tf.squeeze(self.output_layer)
            
            self.loss = tf.squared_difference(self.value, self.target) 
            
            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            
            self.train_op = self.optimizer.minimize(
                self.loss, global_step=tf.contrib.framework.get_global_step())
    
    def predict(self, state, sess=None):
        """
        """
        sess = sess or tf.get_default_session()
        return sess.run(self.value, {self.state: state})

    def update(self, state, target, sess=None):
        """
        """
        sess = sess or tf.get_default_session()
        _, loss = sess.run([self.train_op, self.loss], {self.state: state, self.target: target})
        
        return loss